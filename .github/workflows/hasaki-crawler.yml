name: Hasaki Crawler - Automated Data Pipeline

on:
  # Weekly listing crawl: Every Monday at 1:00 AM UTC (8:00 AM Vietnam)
  # Daily product crawl: Every day at 2:00 AM UTC (9:00 AM Vietnam)
  schedule:
    - cron: '0 1 * * 1'  # Weekly: Mondays at 1:00 AM UTC
    - cron: '0 2 * * *'  # Daily: Every day at 2:00 AM UTC
  
  # Allow manual trigger with workflow selection
  workflow_dispatch:
    inputs:
      crawl_type:
        description: 'Choose crawl type'
        required: true
        type: choice
        options:
          - 'both'        # Run both listing and product crawl
          - 'listing'     # Only listing crawl (Phase 1)
          - 'product'     # Only product crawl (Phase 2)
        default: 'both'

jobs:
  # Job 1: Listing Crawl (Phase 1 - Discovery)
  listing-crawl:
    name: Phase 1 - Listing Crawl
    runs-on: ubuntu-latest
    # Run on: Manual trigger with 'listing'/'both', OR Monday schedule
    if: |
      (github.event_name == 'workflow_dispatch' && (github.event.inputs.crawl_type == 'listing' || github.event.inputs.crawl_type == 'both')) ||
      (github.event_name == 'schedule' && github.event.schedule == '0 1 * * 1')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run listing crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          SUPABASE_SCHEMA: raw
          LOG_LEVEL: INFO
        run: |
          echo "=========================================="
          echo "PHASE 1: LISTING CRAWL"
          echo "=========================================="
          echo "üìã Purpose: Populate listing_api with ALL product IDs"
          echo "‚è±Ô∏è  Estimated time: 10-15 minutes"
          echo "‚öôÔ∏è  Workers: 20 parallel (category-level)"
          echo "üéØ Target: All leaf categories"
          echo "=========================================="
          python crawl_listings.py
          echo "‚úÖ Listing crawl completed!"
      
      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: listing-crawler-logs-${{ github.run_id }}
          path: logs/
          retention-days: 7
      
      - name: Notify on failure
        if: failure()
        run: |
          echo "‚ùå Weekly listing crawl failed!"
          echo "üìä Check logs artifact: listing-crawler-logs-${{ github.run_id }}"

  # Job 2: Product & Review Crawl (Phase 2 - Detail Data)
  product-crawl:
    name: Phase 2 - Product & Review Crawl
    runs-on: ubuntu-latest
    # Run on: Manual trigger with 'product'/'both', OR daily schedule, OR after listing-crawl
    if: |
      (github.event_name == 'workflow_dispatch' && (github.event.inputs.crawl_type == 'product' || github.event.inputs.crawl_type == 'both')) ||
      (github.event_name == 'schedule' && github.event.schedule == '0 2 * * *') ||
      success()
    needs: [listing-crawl]
    # Continue even if listing-crawl is skipped (for daily runs)
    if: always() && (needs.listing-crawl.result == 'success' || needs.listing-crawl.result == 'skipped')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run product & review crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          SUPABASE_SCHEMA: raw
          LOG_LEVEL: INFO
        run: |
          echo "=========================================="
          echo "PHASE 2: PRODUCT & REVIEW CRAWL"
          echo "=========================================="
          echo "üìã Purpose: Crawl product details + reviews"
          echo "üìä Source: Product IDs from listing_api (database)"
          echo "‚è±Ô∏è  Estimated time: 1-2 minutes (for 212 products)"
          echo "‚öôÔ∏è  Workers: 15 product, 6 review (parallel)"
          echo "üéØ Target: Products matching brands.txt"
          echo "üìà Features:"
          echo "   - Incremental snapshot (only changed data)"
          echo "   - Smart review pagination (prevent API bug)"
          echo "   - Socket error retry (3 attempts)"
          echo "   - Direct JSONB comparison (no Python-side hashing)"
          echo "=========================================="
          python crawler.py
          echo "‚úÖ Product & review crawl completed!"
      
      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: product-crawler-logs-${{ github.run_id }}
          path: logs/
          retention-days: 7
      
      - name: Notify on failure
        if: failure()
        run: |
          echo "‚ùå Daily product crawl failed!"
          echo "üìä Check logs artifact: product-crawler-logs-${{ github.run_id }}"

  # Job 3: Summary Report
  summary:
    name: Crawl Summary
    runs-on: ubuntu-latest
    needs: [listing-crawl, product-crawl]
    if: always()
    
    steps:
      - name: Generate summary
        run: |
          echo "=========================================="
          echo "HASAKI CRAWLER - EXECUTION SUMMARY"
          echo "=========================================="
          echo "üïê Run time: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo "üîÄ Trigger: ${{ github.event_name }}"
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "üìã Crawl type: ${{ github.event.inputs.crawl_type }}"
          fi
          echo ""
          echo "Job Results:"
          echo "  üìã Listing Crawl: ${{ needs.listing-crawl.result }}"
          echo "  üì¶ Product Crawl: ${{ needs.product-crawl.result }}"
          echo ""
          if [ "${{ needs.listing-crawl.result }}" == "success" ] && [ "${{ needs.product-crawl.result }}" == "success" ]; then
            echo "‚úÖ All jobs completed successfully!"
          elif [ "${{ needs.listing-crawl.result }}" == "skipped" ] && [ "${{ needs.product-crawl.result }}" == "success" ]; then
            echo "‚úÖ Product crawl completed (listing skipped - daily run)"
          else
            echo "‚ö†Ô∏è  Some jobs failed or were skipped"
          fi
          echo "=========================================="
          echo ""
          echo "üìä Next Scheduled Runs:"
          echo "  - Listing Crawl: Every Monday at 1:00 AM UTC (8:00 AM VN)"
          echo "  - Product Crawl: Every day at 2:00 AM UTC (9:00 AM VN)"
          echo ""
          echo "üîß Manual Trigger: Go to Actions ‚Üí Hasaki Crawler ‚Üí Run workflow"


