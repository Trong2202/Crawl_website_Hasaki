# Hasaki Raw Data Crawler

Professional data engineering crawler for Hasaki.vn product and review data with incremental snapshot strategy and automated GitHub Actions pipeline.

## üéØ Features

### Core Capabilities
- **2-Phase Crawling Architecture**
  - Phase 1: Listing Discovery (weekly) - Kh√°m ph√° t·∫•t c·∫£ products
  - Phase 2: Product & Review Updates (daily) - C·∫≠p nh·∫≠t chi ti·∫øt

- **Incremental Snapshot**
  - Ch·ªâ l∆∞u data khi c√≥ thay ƒë·ªïi
  - Direct JSONB comparison trong PostgreSQL
  - Kh√¥ng c·∫ßn Python-side hashing

- **Smart Review Pagination**
  - T·ª± ƒë·ªông t√≠nh s·ªë pages t·ª´ `total` field
  - Ph√≤ng tr√°nh API bug (l·∫∑p l·∫°i page cu·ªëi)
  - Crawl ch√≠nh x√°c 100% review pages

- **Production-Ready**
  - Retry logic cho socket errors (3 attempts)
  - Parallel processing (15 product workers, 6 review workers)
  - Comprehensive metrics & logging
  - Database-driven product discovery

### Performance
- **Products**: ~23 products/second
- **Reviews**: ~11 pages/second (multi-page per product)
- **Duration**: 1-2 minutes cho 212 products + 750 review pages
- **Efficiency**: 46% storage savings (incremental snapshot)

---

## üìã Table of Contents

1. [Architecture](#-architecture)
2. [Database Schema](#-database-schema)
3. [Installation](#-installation)
4. [Configuration](#-configuration)
5. [Usage](#-usage)
6. [GitHub Actions](#-github-actions)
7. [Monitoring](#-monitoring)
8. [Troubleshooting](#-troubleshooting)

---

## üèóÔ∏è Architecture

### Workflow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    HASAKI CRAWLER PIPELINE                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

PHASE 1: LISTING DISCOVERY (Weekly - Th·ª© 2)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Home API    ‚îÇ ‚Üí Get all categories
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Categories   ‚îÇ ‚Üí Filter leaf categories (no sub-categories)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Listing API  ‚îÇ ‚Üí Parallel crawl (20 workers)
‚îÇ (All Pages)  ‚îÇ ‚Üí Store product IDs + brand IDs ‚Üí listing_api
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

PHASE 2: PRODUCT & REVIEW CRAWL (Daily)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Database    ‚îÇ ‚Üí Query product IDs by brand_ids (0.6s)
‚îÇ (listing_api)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Product API  ‚îÇ ‚Üí Parallel crawl (15 workers)
‚îÇ              ‚îÇ ‚Üí Incremental: Only changed data ‚Üí product_api
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Review API  ‚îÇ ‚Üí Parallel crawl (6 workers)
‚îÇ (Multi-page) ‚îÇ ‚Üí Smart pagination (prevent API bug)
‚îÇ              ‚îÇ ‚Üí Full page JSON ‚Üí review_api
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow

```
API ‚Üí Python Client ‚Üí Supabase Storage ‚Üí PostgreSQL Triggers
                          ‚Üì
                    Direct JSONB
                    Comparison
                          ‚Üì
                  Insert / Skip (NULL)
```

---

## üíæ Database Schema

### Tables

**`crawl_sessions`**
- Track m·ªói l·∫ßn ch·∫°y crawler
- Fields: `session_id`, `api_type`, `started_at`, `finished_at`, `status`

**`home_api`**
- Snapshot c·ªßa home page API (categories, banners, etc.)
- Trigger: Ch·ªâ insert khi data thay ƒë·ªïi

**`listing_api`**
- Product IDs t·ª´ listing pages
- Fields: `product_id`, `brand_id`, `session_id`
- Purpose: Product discovery for Phase 2

**`product_api`**
- Chi ti·∫øt s·∫£n ph·∫©m (title, price, rating, images, etc.)
- Trigger: Ch·ªâ insert khi data thay ƒë·ªïi (incremental snapshot)
- Unique: `(product_id, data)` via trigger

**`review_api`**
- Full page JSON c·ªßa reviews
- Fields: `product_id`, `pages` (page number), `data` (full JSON)
- Unique: `(product_id, pages)` + JSONB comparison via trigger

### Triggers

**Deduplication (Direct JSONB Comparison):**
```sql
-- Product: Check product_id + data
IF EXISTS (SELECT 1 WHERE product_id = NEW.product_id AND data = NEW.data)
    THEN RETURN NULL;

-- Review: Check product_id + pages + data  
IF EXISTS (SELECT 1 WHERE product_id = NEW.product_id AND pages = NEW.pages AND data = NEW.data)
    THEN RETURN NULL;
```

---

## üîß Installation

### Prerequisites
- Python 3.10+
- PostgreSQL 14+ (Supabase)
- Git (for GitHub Actions)

### Local Setup

```bash
# 1. Clone repository
git clone https://github.com/YOUR_USERNAME/hasaki_raw.git
cd hasaki_raw

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ho·∫∑c
venv\Scripts\activate     # Windows

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure environment
cp env.example .env
# Edit .env v·ªõi Supabase credentials
```

### Supabase Setup

```bash
# 1. Copy to√†n b·ªô schema.sql
# 2. V√†o Supabase SQL Editor
# 3. Paste v√† run to√†n b·ªô file

# 4. Verify
SELECT * FROM raw.crawl_sessions LIMIT 1;
```

---

## ‚öôÔ∏è Configuration

### Environment Variables (`.env`)

```env
# Supabase
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-anon-or-service-key
SUPABASE_SCHEMA=raw

# Logging
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR

# API Endpoints (ƒë√£ config trong env.example)
HOME_API=https://hasaki.vn/...
LISTING_API=https://hasaki.vn/...
PRODUCT_API=https://hasaki.vn/...
REVIEW_API=https://hasaki.vn/...
```

### Target Brands (`brands.txt`)

```
105    # CeraVe
1927   # Cocoon
# Add more brand IDs here
```

**T√¨m brand IDs:**
```bash
python find_brands.py
```

---

## üöÄ Usage

### Local Development

```bash
# Phase 1: Listing Crawl (ch·∫°y 1 l·∫ßn/tu·∫ßn)
python crawl_listings.py

# Phase 2: Product & Review Crawl (ch·∫°y h√†ng ng√†y)
python crawler.py
```

### Manual Testing

```bash
# Test specific product
python
>>> from api_client import HasakiAPIClient
>>> client = HasakiAPIClient()
>>> data, meta = client.get_product_detail(84643)
>>> reviews = client.get_product_reviews(84643)
```

---

## ü§ñ GitHub Actions

### Setup

1. **Push code l√™n GitHub**
   ```bash
   git remote add origin https://github.com/YOUR_USERNAME/hasaki_raw.git
   git push -u origin main
   ```

2. **Add Secrets**
   - V√†o: Settings ‚Üí Secrets and variables ‚Üí Actions
   - Add: `SUPABASE_URL`, `SUPABASE_KEY`

3. **Done!** Workflow t·ª± ƒë·ªông ch·∫°y

### Schedule

- **Weekly**: Th·ª© 2, 1:00 AM UTC (8:00 AM VN) - Listing Crawl
- **Daily**: M·ªói ng√†y, 2:00 AM UTC (9:00 AM VN) - Product Crawl

### Manual Trigger

1. **Actions** ‚Üí **Hasaki Crawler**
2. **Run workflow**
3. Ch·ªçn: `both` / `listing` / `product`

**Chi ti·∫øt:** Xem [GITHUB_ACTIONS_SETUP.md](GITHUB_ACTIONS_SETUP.md)

---

## üìä Monitoring

### Check Crawl Status

```sql
-- Session m·ªõi nh·∫•t
SELECT * FROM raw.crawl_sessions 
ORDER BY started_at DESC LIMIT 1;

-- Stats per session
SELECT 
    session_id,
    started_at,
    finished_at,
    status,
    total_items,
    skipped_items,
    EXTRACT(EPOCH FROM (finished_at - started_at)) as duration_seconds
FROM raw.crawl_sessions
ORDER BY started_at DESC;
```

### Check Data Quality

```sql
-- Products per brand
SELECT 
    LEFT(product_id, 2) as brand_prefix,
    COUNT(DISTINCT product_id) as products
FROM raw.product_api
GROUP BY brand_prefix;

-- Review coverage
SELECT 
    COUNT(DISTINCT pa.product_id) as products_with_details,
    COUNT(DISTINCT ra.product_id) as products_with_reviews,
    ROUND(COUNT(DISTINCT ra.product_id)::numeric / 
          COUNT(DISTINCT pa.product_id) * 100, 2) as review_coverage_pct
FROM raw.product_api pa
LEFT JOIN raw.review_api ra ON pa.product_id = ra.product_id;

-- Top reviewed products
SELECT 
    product_id,
    COUNT(*) as review_pages,
    MAX(pages) as last_page,
    MIN(created_at) as first_crawl,
    MAX(created_at) as last_crawl
FROM raw.review_api
GROUP BY product_id
ORDER BY review_pages DESC
LIMIT 10;
```

---

## üîç Troubleshooting

### Common Issues

#### ‚ùå "No products found in database"
```bash
# Ch·∫°y listing crawl tr∆∞·ªõc
python crawl_listings.py
```

#### ‚ùå "Permission denied for schema raw"
```sql
-- Supabase SQL Editor
GRANT ALL ON SCHEMA raw TO anon, authenticated, service_role;
GRANT ALL ON ALL TABLES IN SCHEMA raw TO anon, authenticated, service_role;
GRANT ALL ON ALL FUNCTIONS IN SCHEMA raw TO anon, authenticated, service_role;
```

#### ‚ùå "WinError 10035" (Socket errors)
```python
# Gi·∫£m workers trong crawler.py
MAX_REVIEW_WORKERS = 4  # Gi·∫£m t·ª´ 6 xu·ªëng 4
```

#### ‚ùå Products b·ªã duplicate
```sql
-- Check trigger
SELECT * FROM pg_trigger WHERE tgname = 'trigger_product_change_detection';

-- Recreate trigger (copy t·ª´ schema.sql)
```

### Debug Mode

```bash
# Enable debug logging
export LOG_LEVEL=DEBUG
python crawler.py

# Ho·∫∑c edit .env
LOG_LEVEL=DEBUG
```

---

## üìö Documentation

- [FINAL_FIXES.md](FINAL_FIXES.md) - Chi ti·∫øt t·∫•t c·∫£ fixes
- [API_BUG_FIX.md](API_BUG_FIX.md) - Hasaki API bug & solution
- [GITHUB_ACTIONS_SETUP.md](GITHUB_ACTIONS_SETUP.md) - GitHub Actions guide

---

## üìà Performance Benchmarks

**System:** GitHub Actions (ubuntu-latest, 2 CPU cores)

| Phase | Items | Duration | Throughput | Workers |
|-------|-------|----------|------------|---------|
| Listing | ~3000 products | 10-15 min | 3-5 products/s | 20 parallel |
| Product | 212 products | 9-10s | 22-23 products/s | 15 parallel |
| Review | 750 pages | 60-70s | 10-12 pages/s | 6 parallel |

**Storage Efficiency:** 46% duplicate rate (incremental snapshot working!)

---

## üéâ Credits

Developed by [Your Name]

**Tech Stack:**
- Python 3.10
- Supabase (PostgreSQL 14)
- GitHub Actions
- requests, python-dotenv, supabase-py

---

## üìù License

[Your License] (MIT recommended)

---

## ü§ù Contributing

Contributions welcome! Please:
1. Fork repository
2. Create feature branch
3. Commit changes
4. Push to branch
5. Open Pull Request

---

**Happy Crawling! üöÄ**
