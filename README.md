# Hasaki Raw Data Crawler

Data pipeline tá»± Ä‘á»™ng cho Hasaki.vn vá»›i incremental snapshot vÃ  smart review pagination.

## ğŸ¯ TÃ­nh NÄƒng

- **2-Phase Crawling**: Listing (tuáº§n) + Product & Review (ngÃ y)
- **Incremental Snapshot**: Chá»‰ lÆ°u data thay Ä‘á»•i (JSONB comparison)
- **Smart Pagination**: Tá»± Ä‘á»™ng detect káº¿t thÃºc reviews (trÃ¡nh API bug)
- **Production-Ready**: Retry logic, parallel processing, GitHub Actions

## ğŸ—ï¸ Architecture

```
Weekly:  Home â†’ Categories â†’ Listing (20 workers) â†’ listing_api
Daily:   Database â†’ Product (10 workers) â†’ product_api
                 â†“ Review (20 workers)  â†’ review_api
```

**Workers Allocation:**
- Review: 20 (náº·ng nháº¥t - multi-page per product)
- Listing: 20 (náº·ng - khÃ¡m phÃ¡ toÃ n bá»™)
- Product: 10 (nháº¹ nháº¥t - 1 request per product)

## âš¡ Performance

- **Products**: ~20-25 products/s
- **Reviews**: ~15-20 pages/s
- **Duration**: 1-2 phÃºt (212 products, 750+ review pages)


## ğŸ¤– GitHub Actions

### Setup

1. Push code lÃªn GitHub
2. Settings â†’ Secrets â†’ Add `SUPABASE_URL`, `SUPABASE_KEY`
3. Done! Tá»± Ä‘á»™ng cháº¡y:
   - **Thá»© 2, 1:00 AM UTC**: Listing crawl
   - **Má»—i ngÃ y, 2:00 AM UTC**: Product + Review crawl


## ğŸ’¾ Database Schema

| Table | Purpose | Unique Constraint |
|-------|---------|-------------------|
| `crawl_sessions` | Track má»—i láº§n cháº¡y | - |
| `home_api` | Home page snapshot | `data` (trigger) |
| `listing_api` | Product IDs | `(product_id, brand_id, session_id)` |
| `product_api` | Chi tiáº¿t sáº£n pháº©m | `(product_id, data)` (trigger) |
| `review_api` | Review pages | `(product_id, pages, data)` (trigger) |
cd 'c:\Users\ttron\Documents\A_Project\hasaki_raw'
Get-ChildItem -Recurse -File | Select-Object -ExpandProperty FullName
**Deduplication:** Direct JSONB comparison trong PostgreSQL triggers (khÃ´ng hash Python-side)


```sql
-- Session má»›i nháº¥t
SELECT * FROM raw.crawl_sessions ORDER BY started_at DESC LIMIT 1;

-- Review coverage
SELECT 
    COUNT(DISTINCT pa.product_id) as products,
    COUNT(DISTINCT ra.product_id) as with_reviews,
    ROUND(100.0 * COUNT(DISTINCT ra.product_id) / COUNT(DISTINCT pa.product_id), 1) as coverage_pct
FROM raw.product_api pa
LEFT JOIN raw.review_api ra ON pa.product_id = ra.product_id;

-- Top reviewed
SELECT product_id, COUNT(*) as pages, MAX(pages) as last_page
FROM raw.review_api
GROUP BY product_id
ORDER BY pages DESC
LIMIT 10;
```

## ğŸ”§ Configuration

### Environment (`.env`)

```env
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_KEY=your-key
SUPABASE_SCHEMA=raw

```

### Target Brands (`brands.txt`)

```
105    # CeraVe
1927   # Cocoon
```

TÃ¬m thÃªm brands: `python find_brands.py`

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| "No products found" | `python crawl_listings.py` trÆ°á»›c |
| "Permission denied" | Grant quyá»n trong schema.sql (cuá»‘i file) |
| Socket errors | Giáº£m `MAX_REVIEW_WORKERS` trong `crawler.py` |
| Duplicates | Check triggers trong `schema.sql` |

Debug mode: `export LOG_LEVEL=DEBUG`

## ğŸ“ Project Structure

```
hasaki_raw/
â”œâ”€â”€ crawler.py              # Main: Product & Review crawl
â”œâ”€â”€ crawl_listings.py       # Phase 1: Listing crawl
â”œâ”€â”€ api_client.py           # Hasaki API wrapper
â”œâ”€â”€ supabase_client.py      # Database operations
â”œâ”€â”€ config.py               # Configuration
â”œâ”€â”€ logger.py               # Logging setup
â”œâ”€â”€ schema.sql              # PostgreSQL schema
â”œâ”€â”€ brands.txt              # Target brand IDs
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ .github/workflows/
    â””â”€â”€ hasaki-crawler.yml  # GitHub Actions
```

